{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM,Dropout,Dense,Embedding,Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "train_data_size = 1000\n",
    "test_data_size = 1000\n",
    "DIC_SIZE = 10000\n",
    "MAX_LEN = 1575\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = buildVocabulary(os.getcwd()+'/aclImdb/train/')\n",
    "vocabulary = loadDic(os.getcwd()+'/aclImdb/imdb.vocab')\n",
    "Xtrain, Xtest, ytrain, ytest = loadData(os.getcwd()+'/aclImdb/', vocabulary) # tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_representations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f730a8717634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Logistic Regression Text Classification using sentence representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlogestic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_representations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_representations' is not defined"
     ]
    }
   ],
   "source": [
    "vocabulary = loadDic(os.getcwd()+'/aclImdb/imdb.vocab')\n",
    "\n",
    "\n",
    "# load tokens\n",
    "trn_tokens, test_tokens, trn_labels, test_labels = loadData(os.getcwd()+'/aclImdb/', vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "## Method1: Logistic Regression using word-embeddings sentence representation\n",
    "# load pre-trained 50D word embedding\n",
    "word_embedding = load_word_embedding('glove.6B/glove.6B.200d.txt')\n",
    "\n",
    "\n",
    "# build sentence representations\n",
    "trn_representations = construct_sentence_rep(trn_tokens, word_embedding)\n",
    "test_representations = construct_sentence_rep(test_tokens, word_embedding)\n",
    "\n",
    "\n",
    "# Logistic Regression Text Classification using sentence representations\n",
    "logestic_regression(trn_representations, trn_labels, test_representations, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTM_model()\n",
    "# model.summary()\n",
    "# model.fit(trn_index[:1],trn_labels[:1])\n",
    "# dic = buildVocabulary(os.getcwd()+'/aclImdb/train/')\n",
    "trn_text, test_text, trn_labels, test_labels = loadText(os.getcwd()+'/aclImdb/')\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(list(trn_text)+list(test_text))\n",
    "X_train = tokenizer.texts_to_sequences(trn_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=MAX_LEN)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         30000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 30,219,777\n",
      "Trainable params: 30,219,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 669 samples, validate on 331 samples\n",
      "Epoch 1/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 1.3616 - accuracy: 0.5281\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.58610, saving model to /Users/liuyining/Documents/GitZone/Data Mining/IMDB-Review-Sentiment-Analysis/weights/lstm-01-0.59.hdf5\n",
      "669/669 [==============================] - 45s 67ms/sample - loss: 1.3310 - accuracy: 0.5306 - val_loss: 0.6789 - val_accuracy: 0.5861\n",
      "Epoch 2/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.5825 - accuracy: 0.7891\n",
      "Epoch 00002: val_accuracy did not improve from 0.58610\n",
      "669/669 [==============================] - 44s 66ms/sample - loss: 0.5792 - accuracy: 0.7907 - val_loss: 0.6701 - val_accuracy: 0.5831\n",
      "Epoch 3/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.4237 - accuracy: 0.8094\n",
      "Epoch 00003: val_accuracy improved from 0.58610 to 0.66465, saving model to /Users/liuyining/Documents/GitZone/Data Mining/IMDB-Review-Sentiment-Analysis/weights/lstm-03-0.66.hdf5\n",
      "669/669 [==============================] - 45s 67ms/sample - loss: 0.4193 - accuracy: 0.8146 - val_loss: 0.6599 - val_accuracy: 0.6647\n",
      "Epoch 4/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.1787 - accuracy: 0.9688\n",
      "Epoch 00004: val_accuracy improved from 0.66465 to 0.69486, saving model to /Users/liuyining/Documents/GitZone/Data Mining/IMDB-Review-Sentiment-Analysis/weights/lstm-04-0.69.hdf5\n",
      "669/669 [==============================] - 44s 66ms/sample - loss: 0.1713 - accuracy: 0.9701 - val_loss: 1.1227 - val_accuracy: 0.6949\n",
      "Epoch 5/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 1.6219 - accuracy: 0.8906\n",
      "Epoch 00005: val_accuracy improved from 0.69486 to 0.74018, saving model to /Users/liuyining/Documents/GitZone/Data Mining/IMDB-Review-Sentiment-Analysis/weights/lstm-05-0.74.hdf5\n",
      "669/669 [==============================] - 43s 65ms/sample - loss: 1.6432 - accuracy: 0.8894 - val_loss: 3.0093 - val_accuracy: 0.7402\n",
      "Epoch 6/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 1.7715 - accuracy: 0.8766\n",
      "Epoch 00006: val_accuracy did not improve from 0.74018\n",
      "669/669 [==============================] - 44s 66ms/sample - loss: 1.6948 - accuracy: 0.8819 - val_loss: 1.4181 - val_accuracy: 0.6888\n",
      "Epoch 7/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.0162 - accuracy: 0.9953\n",
      "Epoch 00007: val_accuracy did not improve from 0.74018\n",
      "669/669 [==============================] - 43s 64ms/sample - loss: 0.0161 - accuracy: 0.9955 - val_loss: 1.5181 - val_accuracy: 0.6677\n",
      "Epoch 8/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.0130 - accuracy: 0.9984\n",
      "Epoch 00008: val_accuracy did not improve from 0.74018\n",
      "669/669 [==============================] - 43s 64ms/sample - loss: 0.0125 - accuracy: 0.9985 - val_loss: 1.1222 - val_accuracy: 0.6798\n",
      "Epoch 9/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00009: val_accuracy did not improve from 0.74018\n",
      "669/669 [==============================] - 44s 66ms/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.4023 - val_accuracy: 0.7009\n",
      "Epoch 10/30\n",
      "640/669 [===========================>..] - ETA: 1s - loss: 3.3235e-04 - accuracy: 1.0000\n",
      "Epoch 00010: val_accuracy did not improve from 0.74018\n",
      "669/669 [==============================] - 44s 65ms/sample - loss: 3.1794e-04 - accuracy: 1.0000 - val_loss: 1.6245 - val_accuracy: 0.7009\n",
      "Epoch 11/30\n",
      " 64/669 [=>............................] - ETA: 38s"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e020adfc1580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrn_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# model.fit(np.array(trn_index[:1000]),trn_labels[:1000], validation_split=0.33, epochs=30, batch_size=64, callbacks=callbacks_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml3.7/lib/python3.7/site-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "## Method2: LSTM sequence sentenment classifier\n",
    "trn_text, test_text, trn_labels, test_labels = loadText(os.getcwd()+'/aclImdb/')\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(list(trn_text)+list(test_text))\n",
    "X_train = tokenizer.texts_to_sequences(trn_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=MAX_LEN)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=MAX_LEN)\n",
    "\n",
    "model = LSTM_model()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "filepath= os.getcwd()+\"/weights/lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(np.array(X_train[:1000]),trn_labels[:1000], validation_split=0.33, epochs=30, batch_size=64, callbacks=callbacks_list)\n",
    "# model.fit(np.array(trn_index[:1000]),trn_labels[:1000], validation_split=0.33, epochs=30, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Acc accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Method3: Multivariate Bernoulli Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import math\n",
    "\n",
    "# dic = buildVocabulary(os.getcwd()+'/aclImdb/train/')\n",
    "vocabulary = loadDic(os.getcwd()+'/aclImdb/imdb.vocab')\n",
    "Xtrain, Xtest, ytrain, ytest = loadDataBOW(os.getcwd()+'/aclImdb/', vocabulary) # load BOW representation\n",
    "\n",
    "# thetaPosTrue, thetaNegTrue = naiveBayesBernFeature_train(Xtrain[:1000], ytrain[:1000])\n",
    "# yPredict, Accuracy = naiveBayesBernFeature_test(Xtest[:1000], ytest[:1000], thetaPosTrue, thetaNegTrue)\n",
    "# print(\"Multivariate Bernoulli Naive Bayes Classifier Acc: \",Accuracy)\n",
    "\n",
    "Accuracy = naiveBayesMultinomial_MNBC(Xtrain, ytrain, Xtest, ytest)\n",
    "print(\"Naive Bayes Classifier Acc accuracy =\", Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naiveBayesMultinomial_MNBC(Xtrain, ytrain, Xtest, ytest):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    yPredict = clf.predict(Xtest)\n",
    "    errors = np.sum(np.absolute(yPredict - ytest))\n",
    "    Accuracy = (ytest.shape[0] - errors)/ytest.shape[0]\n",
    "    return Accuracy\n",
    "\n",
    "def naiveBayesBernFeature_train(Xtrain, ytrain):\n",
    "    thetaPosTrue = np.zeros((Xtrain.shape[1],), dtype = float)\n",
    "    thetaNegTrue = np.zeros((Xtrain.shape[1],), dtype = float)\n",
    "    for i in range(len(Xtrain)):\n",
    "        Binary = np.where(Xtrain[i] > 0, 1, 0)\n",
    "        if ytrain[i] == 1:\n",
    "            thetaPosTrue += Binary\n",
    "        elif ytrain[i] == 0:\n",
    "            thetaNegTrue += Binary\n",
    "    thetaPosTrue += 1\n",
    "    thetaNegTrue += 1\n",
    "    thetaPosTrue /= (Xtrain.shape[0]/2 + 2)\n",
    "    thetaNegTrue /= (Xtrain.shape[0]/2 + 2)\n",
    "    return thetaPosTrue, thetaNegTrue\n",
    "\n",
    "\n",
    "def naiveBayesBernFeature_test(Xtest, ytest, thetaPosTrue, thetaNegTrue):\n",
    "    yPredict = []\n",
    "    for X in Xtest:\n",
    "        N = np.sum(X) #total words of X\n",
    "        thetaLogPos = 0.0\n",
    "        thetaLogNeg = 0.0\n",
    "        for i in range(len(X)):\n",
    "            if X[i] > 0:\n",
    "                thetaLogPos += math.log2(thetaPosTrue[i])\n",
    "                thetaLogNeg += math.log2(thetaNegTrue[i])\n",
    "            elif X[i] ==0:\n",
    "                thetaLogPos += math.log2(1-thetaPosTrue[i])\n",
    "                thetaLogNeg += math.log2(1-thetaNegTrue[i])\n",
    "        if thetaLogPos > thetaLogNeg:\n",
    "            yPredict.append(1)\n",
    "        else:\n",
    "            yPredict.append(0)\n",
    "    yPredict = np.array(yPredict)\n",
    "#     print(yPredict, ytest)\n",
    "    errors = np.sum(np.absolute(yPredict - ytest))\n",
    "    Accuracy = (ytest.shape[0] - errors)/ytest.shape[0]\n",
    "    return yPredict, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trn_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-76ccb55a24e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get word index representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrn_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# padding word index representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trn_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# get word index representation\n",
    "# trn_index = [token_to_index(tokens, vocabulary) for tokens in trn_tokens]\n",
    "# test_index = [token_to_index(tokens, vocabulary) for tokens in test_tokens]\n",
    "\n",
    "# # padding word index representation\n",
    "# trn_index = sequence.pad_sequences(trn_index,maxlen=MAX_LEN)\n",
    "# test_index = sequence.pad_sequences(test_index,maxlen=MAX_LEN)\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=100000)\n",
    "# tokenizer.fit_on_texts(list(trn_tokens)+list(test_tokens))\n",
    "# X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(Path):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #tokenizer removing puncturations\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = []\n",
    "    for folder in ['neg','pos']:\n",
    "        for filename in os.listdir(Path+folder):\n",
    "            data = open(Path+folder+'/'+filename, 'r')\n",
    "            raw = data.read()\n",
    "            token = tokenizer.tokenize(raw)\n",
    "            # stop words removal\n",
    "            filtered_token = [w for w in token if not w in stop_words]\n",
    "#             filtered_token = [ps.stem(w) for w in token if not w in stop_words]\n",
    "            # stemming\n",
    "            for i in range(len(filtered_token)):\n",
    "                filtered_token[i] = ps.stem(filtered_token[i])\n",
    "            tokens.extend(filtered_token)\n",
    "    word_counts = collections.Counter(tokens).most_common(DIC_SIZE-1)\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    for word_count in word_counts:\n",
    "        vocab[word_count[0]] = i\n",
    "        i += 1\n",
    "    vocab['UNK'] = i\n",
    "    return vocab\n",
    "\n",
    "def loadDic(Path, skip_stop = False, stem = False, lower = True):\n",
    "    dic = open(Path,'r')\n",
    "    dic = dic.read().split('\\n')\n",
    "    \n",
    "    # stop words removal and stemming\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if stem:\n",
    "        dic = list(set([ps.stem(w) for w in dic]))\n",
    "    if skip_stop:\n",
    "        dic = list(set([w for w in dic if not w in stop_words]))\n",
    "    if lower:\n",
    "        dic = list(set([w.lower() for w in dic]))\n",
    "    else:\n",
    "        dic = list(set(dic))\n",
    "    \n",
    "    \n",
    "    # transfer to dictionary\n",
    "    \n",
    "    filtered_dic = {word: i for i, word in enumerate(dic)}\n",
    "    filtered_dic['UNK'] = len(filtered_dic)\n",
    "    return filtered_dic\n",
    "\n",
    "def loadDataBOW(Path, vocabulary):\n",
    "    voc_size = len(vocabulary)\n",
    "    Xtrain = np.zeros((train_data_size*2,voc_size), dtype=int)\n",
    "    Xtest = np.zeros((train_data_size*2,voc_size), dtype=int)\n",
    "    ytrain = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    ytest = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(Path+'train/pos/')[:train_data_size]):\n",
    "#         Xtrain.append(transfer(Path+'train/pos/'+filename, vocabulary))\n",
    "        Xtrain[i] = transfer(Path+'train/pos/'+filename, vocabulary)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/neg/')[:train_data_size]):\n",
    "        Xtrain[i] = transfer(Path+'train/neg/'+filename, vocabulary)\n",
    "#         Xtrain.append(transfer(Path+'train/neg/'+filename, vocabulary))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/pos/')[:test_data_size]):\n",
    "        Xtest[i] = transfer(Path+'test/pos/'+filename, vocabulary)\n",
    "#         Xtest.append(transfer(Path+'test/pos/'+filename, vocabulary))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/neg/')[:test_data_size]):\n",
    "        Xtest[i] = transfer(Path+'test/neg/'+filename, vocabulary)\n",
    "#         Xtest.append(transfer(Path+'test/neg/'+filename, vocabulary))\n",
    "\n",
    "#     Xtrain, Xtest = np.array(Xtrain), np.array(Xtest)\n",
    "    Xtrain, ytrain = shuffle(Xtrain, ytrain, random_state=0)\n",
    "    Xtest, ytest = shuffle(Xtest, ytest, random_state=0)\n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "\n",
    "\n",
    "def loadText(Path):\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    ytrain = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    ytest = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    \n",
    "    for i, filename in enumerate(os.listdir(Path+'train/pos/')[:train_data_size]):\n",
    "        data = open(Path+'train/pos/'+filename,'r')\n",
    "        raw = data.read().lower()\n",
    "        Xtrain.append(raw)\n",
    "#         max_len = max(len(tokens),max_len)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/neg/')[:train_data_size]):\n",
    "        data = open(Path+'train/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        Xtrain.append(raw)\n",
    "#         max_len = max(len(tokens),max_len)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/pos/')[:test_data_size]):\n",
    "        data = open(Path+'test/pos/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        Xtest.append(raw)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/neg/')[:test_data_size]):\n",
    "        data = open(Path+'test/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        Xtest.append(raw)\n",
    "        \n",
    "#     print(max_len)\n",
    "\n",
    "    Xtrain, ytrain = shuffle(Xtrain, ytrain, random_state=0)\n",
    "    Xtest, ytest = shuffle(Xtest, ytest, random_state=0)\n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "\n",
    "def loadData(Path, vocabulary):\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    ytrain = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    ytest = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #tokenizer removing puncturations\n",
    "#     max_len = 0\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/pos/')[:train_data_size]):\n",
    "        data = open(Path+'train/pos/'+filename,'r')\n",
    "        raw = data.read().lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtrain.append(tokens)\n",
    "#         max_len = max(len(tokens),max_len)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/neg/')[:train_data_size]):\n",
    "        data = open(Path+'train/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtrain.append(tokens)\n",
    "#         max_len = max(len(tokens),max_len)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/pos/')[:test_data_size]):\n",
    "        data = open(Path+'test/pos/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtest.append(tokens)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/neg/')[:test_data_size]):\n",
    "        data = open(Path+'test/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtest.append(tokens)\n",
    "        \n",
    "#     print(max_len)\n",
    "\n",
    "    Xtrain, ytrain = shuffle(Xtrain, ytrain, random_state=0)\n",
    "    Xtest, ytest = shuffle(Xtest, ytest, random_state=0)\n",
    "    return Xtrain, Xtest, ytrain, ytest\n",
    "\n",
    "\n",
    "def transfer(fileDj, vocabulary):\n",
    "    data = open(fileDj,'r')\n",
    "    BOWDj = np.zeros((len(vocabulary),), dtype = int)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #tokenizer removing puncturations\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    raw = data.read()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # stop words removal\n",
    "    filtered_token = [ps.stem(w) for w in tokens if not w in stop_words]\n",
    "#     # stemming\n",
    "#     for i in range(len(filtered_token)):\n",
    "#         filtered_token[i] = ps.stem(filtered_token[i])\n",
    "    # transfer to BOW\n",
    "    for token in filtered_token:\n",
    "        if token in vocabulary:\n",
    "            BOWDj[vocabulary[token]] += 1\n",
    "        else:\n",
    "            BOWDj[vocabulary['UNK']] += 1\n",
    "    \n",
    "    return BOWDj\n",
    "\n",
    "\n",
    "def token_to_index(tokens, vocabulary):\n",
    "    lower_token = [w.lower() for w in tokens]\n",
    "    index = [vocabulary.get(token, vocabulary['UNK']) for token in lower_token]\n",
    "    #add <stop> to the last position\n",
    "    index.append(len(vocabulary)+1)\n",
    "    #add <start> to the first position\n",
    "    index1 = [len(vocabulary)]\n",
    "    index1.extend(index)\n",
    "    return index1\n",
    "\n",
    "\n",
    "def loadWordIndex(Path, vocabulary):\n",
    "    voc_size = len(vocabulary)\n",
    "    Xtrain = []\n",
    "    Xtest = []\n",
    "    ytrain = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    ytest = np.concatenate((np.ones(train_data_size, dtype=int),np.zeros(train_data_size, dtype=int)), axis = 0)\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+') #tokenizer removing puncturations\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/pos/')[:train_data_size]):\n",
    "        data = open(Path+'train/pos/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtrain.append(token_to_index(tokens, vocabulary))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'train/neg/')[:train_data_size]):\n",
    "        data = open(Path+'train/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtrain.append(token_to_index(tokens, vocabulary))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/pos/')[:test_data_size]):\n",
    "        data = open(Path+'test/pos/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtest.append(token_to_index(tokens, vocabulary))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(Path+'test/neg/')[:test_data_size]):\n",
    "        data = open(Path+'test/neg/'+filename,'r')\n",
    "        raw = data.read()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        Xtest.append(token_to_index(tokens, vocabulary))\n",
    "\n",
    "    return Xtrain, Xtest, ytrain, ytest    \n",
    "\n",
    "\n",
    "def load_word_embedding(Path):\n",
    "    file = open(Path, 'r')\n",
    "    embedding = {}\n",
    "    for line in file:\n",
    "        line = line.split()\n",
    "        embedding[line[0]] = np.array(line[1:], dtype = float)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def construct_sentence_rep(data, word_embedding, dim=50):\n",
    "    sentence_reps = []\n",
    "    for tokens in data:\n",
    "        current_res = np.zeros((dim,), dtype=float)\n",
    "        for token in tokens:\n",
    "            if token in word_embedding:\n",
    "                current_res += word_embedding[token]\n",
    "            else:\n",
    "                current_res += word_embedding['unk']\n",
    "        if len(tokens)==0: \n",
    "            current_res = word_embedding['unk']\n",
    "        else:\n",
    "            current_res /= len(tokens)\n",
    "        sentence_reps.append(current_res)\n",
    "    return np.array(sentence_reps, dtype=float)\n",
    "\n",
    "\n",
    "def logestic_regression(trn_x, trn_labels, dev_x, dev_labels):\n",
    "    # Define a LR classifier\n",
    "    classifier = LogisticRegression(random_state=0,tol=0.0001,solver='liblinear',\n",
    "                                    multi_class='auto',C=0.1, penalty='l1') #0.647\n",
    "#     classifier = LogisticRegression(random_state=0,tol=0.0002,solver='liblinear',\n",
    "#                                     multi_class='auto',C=0.1, penalty='l1') \n",
    "#     classifier = LogisticRegression(verbose=1, C=5, penalty='l2')\n",
    "    classifier.fit(trn_x, trn_labels)\n",
    "\n",
    "    # Measure the performance on dev data\n",
    "    # print(\"Training accuracy = %f\" % classifier.score(trn_x, trn_labels))\n",
    "    print(\"Dev accuracy = \", classifier.score(dev_x, dev_labels))\n",
    "    \n",
    "    \n",
    "def LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=100000,output_dim=300))\n",
    "    model.add(LSTM(units=128,dropout=0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
